\blu{2-22: We are at the final lemma, Lemma~\ref{lem:rip-step2-1} in the Restricted Invertiblity Theorem.}

\ig{images/7-1}{.25}

%Let me just remind you were we were. We were at the final lemma in the Restricted Invertiblity Theorem. We have a linear map $A: \R^m \to \R^n$, and have $\{Ae_j\}_{j = 1}^m$ linearly independent, and denote 
%\[
%F_j = \left(\txtn{span}(\{Ae_j\}_{i \neq j})\right)^{\perp}
%\]
%and then denote $M = \max_{i \leq j \leq m} \frac{1}{\|\txtn{Proj}_{F_1}Ae_j\|}$. 
%
%We have reduced everything to the following lemma: 
%
%\begin{lem} \llabel{lem:SS-induct} (Final lemma). \\
%Suppose $\sigma \subseteq \{1, \cdots, m\}$ with $t \geq 0$ an integer. Then there exists $\tau \subseteq \sigma$ with $|\tau| \geq (1 - \frac{1}{2^t})|\sigma|$ such that if we denote $\theta = \tau \cup \left(\{1, \cdots, m\} \setminus \sigma\right)$, then 
%\[
%\sum_{i \in \tau} |a_i| \le 2^{t/2} M \sqrt{|\sigma|} \|\sum_{i \in \theta}a_iAe_i\|_2 
%\]
%\end{lem}

The proof of this is an inductive application of the Sauer-Shelah lemma. A very important idea comes from Giannopoulos. If you naively try to use Sauer-Shelah, it won't work out. We will give a stronger statement of the previous lemma which we can prove by induction. 

\begin{lem}[Stronger version of Lemma~\ref{lem:rip-step2-1}] \llabel{lem:SS-induct-stronger}
Take $m, n \in \mathbb{N}$, $A: \R^m \to \R^n$ a linear operator such that $\{Ae_j\}_{j = 1}^m$ are linearly independent. Suppose that $k \geq 0$ is an integer and $\sigma \subseteq \{1, \ldots, m\}$. Then there exists $\tau \subseteq \sigma$ with $|\tau| \geq (1 - \frac{1}{2^k})|\sigma|$ such that for every $\theta \supseteq \tau$ for all $a \in \R^m$ we have
\beq{eq:rip21-1}
 \sum_{i \in \tau} |a_i| \leq M\sqrt{|\sigma|}\left(\sum_{r = 1}^k 2^{r/2}\right)\left\|\sum_{i \in \theta} a_i Ae_i\right\|_2 + (2^k - 1)\sum_{i \in \theta \cap (\sigma \setminus \tau)} |a_i|
\eeq
\end{lem}

\ig{images/7-2}{.25}

Lemma~\ref{lem:rip-step2-1} (all we need to complete the proof of the Restricted Invertibility Principle) is the case where $\theta = \tau \cup \{1, \cdots, m\}\setminus \sigma$, $t = k$. 

We prove the stronger version via induction on $k$. 
\begin{proof}
As $k$ becomes bigger, we're allowed to take more of the set $\sigma$. The idea is to use Sauer-Shelah, taking half of $\sigma$, and then half of what remains at each step.

For $k = 0$, the statement is vacuous, because we can take $\tau$ to be the empty set. By induction, assume that the statement holds for $k$: we have found $\tau \subseteq \sigma$ such that $|\tau| \geq (1 - \frac{1}{2^k})|\sigma|$ and~\eqref{eq:rip21-1} holds for every $\tau \subseteq \theta$. If $\sigma = \tau$ already, then $\tau$ satisfies~\eqref{eq:rip21-1} for $k + 1$ as well, so WLOG $|\sigma \setminus \tau| > 0$. Now define $v_j$ is the projection 
\[
v_j = \frac{\Proj_{F_j} Ae_j}{\left\|\Proj_{F_j} Ae_j\right\|_2^2}
\]
Then $\langle v_i, Ae_j \rangle = \delta_{ij}$ by definition since $(v_i)$ is a dual basis for the $Ae_j$'s. 

Now we want to user Sauer-Shelah so we're going to define a certain subset of the cube. Define 
\[
\Omega = \set{\epsilon \in \{\pm 1\}^{\sigma \setminus \tau}}{ \left\|\sum_{i \in \sigma \setminus \tau} \epsilon_i v_i \right\|_2 \leq M\sqrt{2|\sigma \setminus \tau|}}\]
which is an ellipsoid intersected with the cube (it is not a sphere since the $v_i$s are not orthogonal).  Then we have 
\bal
M^2 |\sigma \setminus \tau| &\geq \sum_{i \in \sigma \setminus \tau} \frac{1}{\|\txtn{Proj}_{F_j}Ae_j\|^2} = \sum_{j \in \sigma \setminus \tau} \|v_j\|_2^2\\
&= \frac{1}{2^{|\sigma \setminus \tau|}} \sum_{\epsilon \in \{\pm 1\}^{\sigma \setminus \tau}} \ve{ \sum_{j \in \sigma \setminus \tau} \epsilon_j v_j }_2^2.
\end{align*}
where the last step is true for any vectors (sum the squares and the pairwise correlations disappear). 

Using Markov's inequality this is
\[
\geq \frac{1}{2^{|\sigma \setminus \tau|}} \left(2^{|\sigma \setminus \tau|} - |\Omega| \right)M^2 2|\sigma \setminus \tau|
\]
which gives 
\[
|\Omega| > 2^{|\sigma \setminus \tau| - 1}
\]

Then by Sauer-Shelah (Lemma~\ref{lem:saushel}, there exists $\beta \subseteq \sigma \setminus \tau$ such that 
\[
\Proj_{\R^{\beta}} \Omega = \{\pm 1 \}^{\beta}
\]
and 
\[
|\beta| \geq \frac{1}{2}|\sigma \setminus \tau|
\]

Define $\tau^* = \tau \cup \beta$. We will show that $\tau^*$ satisfies the inductive hypothesis with $k + 1$. Each time we find a certain set of coordinates to add to what we have before. $|\tau^*|$ is the correct size because 
\[
|\tau^*| = |\tau| + |\beta| \geq |\tau| + \frac{|\sigma| - |\tau|}{2} = \frac{|\tau| + |\sigma|}{2} \geq \left(1 - \frac{1}{2^{k + 1}}\right)|\sigma|
\]
where we used that $|\tau| \geq \left(1 - \frac{1}{2^{k}}\right)|\sigma|$. So at least $\tau^*$ is the right size. 

Now, suppose $\theta \supseteq \tau^*$. For every $a \in \R^m$, we claim there exists some $\epsilon \in \Omega$ such that $\forall j \in \beta$ such that $\epsilon_j = \txtn{sign}(a_j)$. For any $\beta$, we can find some vector in the cube that has the sign pattern of our given vector $a$. What does being in $\Omega$ mean? It means that at least the dual basis is small there. $\epsilon \in \Omega$ says that 
\beq{eq:step21-2}
\|\sum_{i \in \sigma \setminus \tau} \epsilon_i v_i\|_2 \leq M \sqrt{2|\sigma \setminus \tau|} \leq \frac{M\sqrt{2|\sigma|}}{2^{k/2}}
\eeq
That was how we chose our ellipsoid. %So we have a bound for just $\tau$ already, now let's do it with the addition of $\beta$. Now
\[
\sum_{i \in \beta} |a_i| = \an{ \sum_{i \in \beta} a_iAe_i, \sum_{i \in \sigma \setminus \tau} \epsilon v_i }
\]
because the $v_i$'s are a dual basis so $\an{Ae_i,v_j}=\de_{ij}$, and $\be \subeq \si\bs \tau$. We only know the $\epsilon_i$ are the signs when you're inside $\beta$.  This equals 
\[
= \an{ \sum_{i \in \theta} a_iAe_i, \sum_{i \in \sigma \setminus \tau} \epsilon v_i} - \sum_{i \in (\theta \setminus \beta)\cap (\sigma \setminus \tau)} \epsilon_ia_i
\]
Note that $(\theta \setminus \beta)\cap (\sigma \setminus \tau) = \theta \cap (\sigma \setminus \tau^*)$. In this set we can't control the signs $\epsilon_i$. By Cauchy-Schwarz and~\eqref{eq:step21-2}, this is
\bal
&\leq \left\| \sum_{i \in \theta} a_i Ae_i\right\|_2 \cdot \left\| \sum_{i \in \sigma \setminus \tau} \epsilon v_i \right\|_2 + \sum_{i \in \theta \cap (\sigma \setminus \tau^*)} |a_i|\\
&\leq \left\| \sum_{i \in \theta} a_iAe_i \right\|_2 \cdot \frac{M\sqrt{2|\sigma|}}{2^{k/2}} + \sum_{i \in \theta \cap (\sigma \setminus \tau^*)} |a_i|.
\end{align*}
Because the conclusion of Sauer-Shelah told us nothing about the signs of $\epsilon_i$ outside $\be$, so we just take the worst possible thing. 

Summarizing,
\[
\sum_{i \in \beta} |a_i| \leq \frac{M\sqrt{2|\sigma|}}{2^{k/2}}\left\|\sum_{i \in \theta} a_iAe_i\right\|_2 + \sum_{i \in \theta \setminus (\sigma \setminus \tau^*)} |a_i|
\]
Using the inductive step, 
\bal
\sum_{i \in \tau^*} |a_i| &= \sum_{i \in \tau} |a_i| + \sum_{i \in \beta} |a_i| 
\\
&\leq M\sqrt{|\sigma|} \alpha_k \left\| \sum_{i \in \theta} a_iAe_i\right\|_2 + \left(2^k - 1\right)\sum_{i \in \theta \cap (\sigma \setminus \tau)}|a_i| + \sum_{i \in \beta} |a_i|
\\
&= \alpha_k \sqrt{|\sigma|}\left\| \sum_{i \in \theta} a_iAe_i\right\|_2 + \left(2^k - 1\right)\sum_{i \in \theta \cap (\sigma \setminus \tau^*)} |a_i| + 2^k \sum_{i \in \beta} |a_i|
\end{align*}
In the last step, we moved $\beta$ from the second to the third term. 
Now use what we got before for the bound on $\sum_{i \in \beta}|a_i|$ and plug it in to get
\[
\leq \left(\alpha_k + 2^{(k + 1)/2}\right)\sqrt{|\sigma|}\left\|\sum_{i \in \theta} a_iAe_i\right\|_2 + \left(2^{k + 1} - 1\right) \sum_{i \in \theta \cap (\sigma \setminus \tau^*)} |a_i|
\]
which is exactly the inductive hypothesis.\footnote{I looked through the original Gianpopoulous paper, and it was clear he tried out many many things to find which inductive hypothesis makes everything go through cleanly. You want to bound an $l_1$ sum from above, so you want to use duality, and then use Sauer-Shelah to get signs such that the norm of the dual-basis is small. }
\end{proof}

\begin{rem}[Algorithmic Sauer-Shelah]
We only used Sauer-Shelah~\ref{lem:saushel} for  intersecting cubes with an ellipsoid, so to make RIP algorithmic, we need an algorithm for finding these particular intersections. Moreover, the ellipsoids are big is because we ask that the 2-norm is at most $\sqrt{2}$ times the expectation. An efficient algorithm probably exists. 

Afterwards, wecould ask for higher dimensional shapes. I've seen some references that worked for Sauer-Shelah when sets were of a special form, namely of size $o(n)$. This is something more geometric. I don't think there's literature about Sauer-Shelah for intersection of surfaces with small degree. This is a tiny motivation to do it, but it's still interesting independently. 
\end{rem}


\chapter{Bourgain's Discretization Theorem}

\section{Outline}

\blu{We will prove Bourgain's Discretization Theorem. This will take maybe two weeks, and has many interesting ingredients along the way.} 
By doing this, we will also prove Ribe's theorem, which is what we stated at the beginning. 

Let's remind ourselves of the definition.

\begin{df*}[Definition~\ref{df:disc-mod}, discretization modulus]
$(X, \|\cdot\|_X), (Y, \|\cdot\|_Y)$ are Banach spaces. Let $\epsilon \in (0, 1)$. Then $\delta_{X \hookrightarrow Y}(\epsilon)$ is the supremum over $\delta > 0$ such that for every $\delta$-net $\cal N_{\de}$ of the unit ball of $X$, the distortion 
\[
C_Y(X) \leq \frac{C_Y(\cal N_{\de})}{1 - \epsilon}
\]
\end{df*}
$C_Y$ is smallest bi-Lipschitz distortion by which you can embed $X$ into $Y$. There are ideas required to get $1 - \epsilon$. For example, for $\epsilon = \rc2$, the equation says is that if we succeed in embedding a $\delta$-net into $Y$, then we succeeded in the full space with a distortion twice as much. A priori it's not even clear that there exists such a $\delta$. There's a nontrivial compactness argument needed to prove its existence. We will just prove bounds on it assuming it exists. 

Now, Bourgain's discretization theorem says 
\begin{thm*}[Bourgain's discretization theorem~\ref{thm:bdt}]
If dim$(X) = n$, dim$(Y) = \infty$, then 
\[
\delta_{X \hra Y}(\epsilon) \geq e^{-\left(\frac{n}{\epsilon}\right)^{C*n}}
\]
for $C$ a universal constant. 
\end{thm*}
%The way to read this is: If $\delta$ is bigger than this number which is only dependent on $n$, then given any Banach spaces with dimension $n$, there are mappings with this granularity for any such spaces. 

\begin{rem}
It doesn't matter what the maps are for the $\cal N_\de$, the proof will give a linear mapping and we won't end up needing them. Assuming linearityp in the definition is not necessary. 


Rademacher's theorem says that any mapping $\R^n \to \R^n$ is differentiable almost everywhere with bi-Lipschitz derivative. Yu can extend this to $n = \infty$, but you need some additional properties: $Y$ must be thedual space, and the limit needs to be in the weak-$*$ topology. In this topology there exists a sequence of norm-$1$ vectors that tend to $0$. 
%A weak $*$ limit of a function can degenerately become $0$, the upper bound is not the issue. But you can prove that this doesn't happen almost everywhere.
Almost everywhere, this doesn't happen. The Principle of Local Reflexivity  says $Y^{**}$ and $Y$ are not the same for infinite dimensional $Y$. The double dual of all sequences which tend to $0$ is $L_{\infty}$, a bigger space, but the difference between these never appears in finite dimensional phenomena. 
\end{rem}

\subsection{Reduction to smooth norm}
From now on, $B_X = \left\{x \in X: \|X\|_X \leq 1 \right\}$ denotes the ball, and $S_X = \partial B = \left\{x \in X: \|X\|_X = 1\right\}$ denotes the boundary. 

Later on we will be differentiating things without thinking about it, so we first prove that we can assume $\ved_X$ is smooth on $X \setminus \{0\}$. 

\begin{lem} For all $\delta \in (0, 1)$ there exists some $\delta$-net of $S_X$ with $|\cal N_{\de}| \leq \left(1 + \frac{2}{\delta}\right)^n$. 
\end{lem}
\begin{proof}
Let $\cal N_{\de} \subseteq S_X$ be maximal with respect to inclusion such that $\|x - y\|_X > \delta$ for every distinct $x, y \in \cal N_{\de}$. Maximality means that if  $z \in S_{X}$, there exists $x \in \cal N_{\de}$, $\|x - y\|_X \leq \delta$ (otherwise $N_\de\cup \{z\}$ is a bigger $\de$-net). The balls $\{x + \frac{\delta}{2}B_X\}_{x \in \cal N_{\de}}$ are pairwise disjoint and contained in $1 + \frac{\delta}{2}B_X$. Thus
\[
\txtn{vol}\pa{\pa{1 + \frac{\delta}{2}}B_X} \geq \sum_{X \in M_{\delta}} \txtn{vol}\pa{x + \frac{\delta}{2}B_X}
\]
\[
\left(1 + \frac{\delta}{2}\right)^n\txtn{vol}(B_X) = |\cal N_{\de}|\left(\frac{\delta}{2}\right)^n\txtn{vol}(B_X)
\]
\footnote{There are non-sharp bounds for the smallest size of a $\delta$-net. There is a lot of literature about the relations between these things, but we just need an upper bound. }
\end{proof}

\begin{proof}[Reduction to smooth norm]
Let $\cal N_\de$ be a $\delta$-net of $S_X$ with $|\cal N_{\de}| = N \leq (1 + \frac{2}{\delta})^n$. 
Given $z\in \cal N_\de$, by Hahn-Banach we can choose $z^*$ such that 
\begin{enumerate}
\item
$\an{z,z^*} = 1$
\item
$\ve{z^*}_{X^*} = 1$.
\end{enumerate}
Let $k$ be an integer such that $N^{1/(2k)} \leq 1 + \delta$. Then define 
\[
\|x\| := \left(\sum_{z \in \cal N_{\de}} \langle z^*, x \rangle^{2k}\right)^{1/(2k)}
\]
Each term satisfies $|\langle z^*, x \rangle| \leq \|x\|_X$, so we know 
\[\|x\| \leq N^{1/2k}\|x\|_X \leq (1 + \delta)\|x\|_X.\] 
For the lower bound, if $x \in S_X$, then choose $z \in \cal N_{\de}$ such that $\|x - z\|_X \leq \delta$. Then $1 - \langle z^*, x \rangle = \langle z^*, z - x \rangle \leq \|z - x\| \leq \delta$, so $\langle z^*, x \rangle \geq 1 - \delta$, giving 
\[
\ve{x}\ge (1-\de)\ve{x}_X.
\] 
Thus any norm is up to $1 + \delta$ some equal to a smooth norm. %$\delta$ was arbitrary, if you prove for Bourgain, you prove it for any norm, and now without loss of generality I can differentiate.
$\de$ was arbitrary, so without loss of generality we can assume in the proof of Bourgain embedding that the norm is smooth.
\end{proof}


The strategy of to prove Bourgain's discretization theorem is as follows. Given a $\delta$-net $\cal N_{\de} \subseteq B_X$ with $\delta \leq e^{-(n/\epsilon)^{Cn}}$, and we know $\exists $ $f: \cal N_{\de} \to Y$ such that $\frac{1}{D}\|x - y\|_X \leq \|f(x) - f(y)\|_Y \leq \|x - y\|_X$, i.e., we can embed with distortion $D$. 
%Our goal is to show that if $\delta < e^{-(D/\epsilon)^{Cn}}$, then there exists a linear operator $T: X \to Y$ such that $\|T\|\cdot\|T^{-1}\| \leq 1 + 20\epsilon$. 
%he switches to $D$ later---which notation is better.
Our goal is to show that if $\de\le e^{-D^{Cn}}$ then this implies that there exists $T:X\to Y$ linear operator invertible $\ve{T}\ve{T^{-1}}\lesssim D$. 

The steps are as follows. 
\begin{enumerate}
\item
Find the correct coordinate system (John ellipsoid), which will give us a dot product structure and a natural Laplacian. (We will need a little background in convex geometry. )
\item
A priori $f$ is defined on the net. Extend $f$ to the whole space in a nice way (Bourgain's extension theorem) which doesn't coincide with the function on the net, but is not too far away from it.
\item
Solve the Laplace equation. Start with some initial condition, and then evolve $f$ according to the Poisson semigroup. This extended function is smooth the instant it flows a little bit away from the discrete function.
\item
There is a point where the derivative satisfies what we want: The point exists by a pigeonhole style argument, but we won't be able to give it explicitly. This comes from estimates of the Poisson kernel. We will use Fourier analysis. 
\end{enumerate}
