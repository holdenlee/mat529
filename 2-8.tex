\blu{2-8-16}

%\begin{thm}[Spielman-Srivastava]
%For $k<\text{srank}(A)=\pf{\ve{A}_{S_2}^2}{\ve{A}_{S_{\iy}}}^2$, $k=(1-\ep)\text{srank}(A)$. 
%$\exists \si\subeq \{1,\ldots, m\}, |\si|=k$, ...
%%,/\op&2. 
%%constant prop of m/ ... squred. 
%%bourgain-tz up to stble rank.
%%shatten root m.
%\end{thm}

Now we will go backwards a bit, and talk about a less general result. After Theorem~\ref{thm:ss}, 
a subsequent theorem gave the same theorem but instead of the stable rank, used something better.
\begin{thm}[Marcus, Spielman, Srivastava]\llabel{thm:mss4}
If 
\[
k<\rc4 \pf{\ve{A}_{S_2}}{\ve{A}_{S_4}}^4,
\]
there exists $\si\subeq \{1,\ldots, m\}$, $|\si|=k$ such that
\[
\ve{(AJ_{\si})^{-1}}_{S_{\iy}} \lesssim \fc{\sqrt m}{\ve{A}_{S_2}}.
\]
%stable rank. 
%got strange norms.
\end{thm}
A lot of these quotients of norms started popping up in people's results. The correct generalization is the following notion.
\begin{df}
For $p>2$, define the \ivocab{stable $p$th rank} by \[\text{srank}_p(A)= \pf{\ve{A}_{S_2}}{\ve{A}_{S_p}}^{\fc{2p}{p-2}}.\]
\end{df}
\begin{exr}
Show that if $p\ge q>2$, then
\[
\text{srank}_p(A) \le \text{srank}_q(A).
\]
(Hint: Use H\"older's inequality.)
\end{exr}
Now we would like to prove how Theorem~\ref{thm:gen-srank} generalizes the previously listed results: 
\begin{proof}[Proof of Generalizability of Theorem~\ref{thm:gen-srank}]
Using H\"older's inequality with $\fc p2$,
\bal
\ve{A}_{S_2}^2 & =\sum_{j=1}^m s_j(A)^2\\
&=\sum_{j=1}^{r-1} s_j(A)^2 + \sum_{j=r}^m s_j(A)^2\\
&\le (r-1)^{1-\fc 2p} \pa{\sum_{j=1}^{r-1} s_j(A)^p}^{\fc 2p} + \sum_{j=r}^m s_j(A)^2\\
&\le (r-1)^{1-\fc 2p} \ve{A}_{S_p}^2  + \sum_{j=r}^m s_j(A)^2\\
\sum_{j=r}^m s_j(A)^2 & \ge 
\ve{A}_{S_2}^2 \pa{1-(r-1)^{-\fc 2p}\fc{\ve{A}_{S_p}^2}{\ve{A}_{S_{2}}^2}}\\
&=\ve{A}_{S_2}^2 \pa{1-\pf{r-1}{\text{srank}_p(A)}^{1-\fc2p}}
\end{align*}
Now we can plug the previous calculation into Theorem~\ref{thm:gen-srank} to demonstrate the way the new theorem generalizes the previous results: 
\bal
\ve{(AJ_\si)^{-1}}&\lesssim\min_{k+1\le r\le \rank(A)} \sfc{mr}{(r-k)\ve{A}_{S_2}^2 \pa{1-\pf{r-1}{\text{srank}_p(A)}^{1-\fc 2p}}}\\
&=\fc{\sqrt m}{\ve{A}_{\iy}}\min_{k+1\le r\le \rank(A)} \sfc{r}{(r-k)\pa{1-\pf{r-1}{\text{srank}_p(A)}^{1-\fc 2p}}}
\end{align*}
This equation implies all the earlier theorems.
%\fixme{How did we get the last 2 lines?}
\end{proof}
To optimize, fix the stable rank, differentiate in $r$, and set to 0. All theorems in the literature follow from this theorem; in particular, we get all the bounds we got before. %3, 2.1.
There was nothing special about the number 4 in Theorem~\ref{thm:mss4}; this is about the distribution of the eigenvalues. 

\section{Ky Fan maximum principle}\llabel{sec:kf}
We'll be doing linear algebra. It's mostly mechanical, except we'll need this lemma.
\begin{lem}[Ky Fan maximum principle]\index{Ky Fan maximum principle}\llabel{lem:kf}
Suppose that $P:\R^m\to \R^m$ is a rank $k$ orthogonal projection. Then
\[
\Tr(A^*AP ) \le \sum_{i=1}^k s_i(A)^2
\]
where $s_i(A)$ are the singular values, i.e., $s_i(A)^2$ are the eigenvalues of $B:=A^*A$.
\end{lem}

%%%%%%%%
\blu{This material was given in class on 2-15.} 
This lemma follows from the following general theorem.
\begin{thm}[Convex function of dot products acheives maximum at eigenvectors] %Ky-Fan maximum principle. 
\llabel{thm:eignmax}
Let $B: \R^n \to \R^n$ be symmetric, and let $f: \R^n \to \R$ be a convex function. Then for every orthonormal basis $u_1, \ldots, u_n \in \R^n$, there exists a permutation $\pi$ such that 
\beq{eq:kf0}
f(\langle Bu_1, u_1 \rangle, \ldots, \langle Bu_n, u_n \rangle) \leq f(\lambda_{\pi(1)}, \ldots, \lambda_{\pi(n)})
\eeq
\end{thm}
Essentially, we're saying using the eigenbasis maximizes the convex function. 
\begin{rem}
We can weaken the condition on $f$ to the following: 
for every $i < j$, $t \mapsto f(x_1, \ldots, x_i + t, x_{i + 1}, \ldots, x_{j - 1}, x_{j} - t, x_{j + 1}, \ldots, x_n)$ is convex as a function of $t$. 
If $f$ is smooth, this is equivalent to the second derivative in $t$ being $\ge 0$:
\beq{eq:kf1}
\frac{\partial^2 f}{\partial x_i^2} + \frac{\partial^2 f}{\partial x_j^2} - 2 \frac{\partial^2 f}{\partial x_i\partial x_j} \geq 0 
\eeq
In other words, you just need that the Hessian is positive semidefinite. (Above, we wrote the determinant of the Hessian on the $x_i-x_j$ plane. This being true for all pairs $i,j$ is the same as the Hessian being positive definite.) 
\end{rem}
\begin{proof}
We may assume\wog that
\begin{enumerate}
\item
$f$ is smooth. If not, convolute with a good kernel.
\item
%Without loss of generality, we can give a strict inequality instead: 
Strict inequality holds:
\[
\frac{\partial^2 f}{\partial x_i^2} + \frac{\partial^2 f}{\partial x_j^2} - 2 \frac{\partial^2 f}{\partial x_i\partial x_j} > 0 .
\]
To see this, note we can take any $\ep > 0$ and perturb $f(x)$ into $f(x) + \ep \|x\|_2^2$. The inequality to prove~\eqref{eq:kf0} is also perturbed by this slight change, and taking $\ep \to 0$ gives the desired inequality.
\end{enumerate}

Now let $u_1, \ldots, u_n$ be an orthonormal basis at which $f(\langle Bu_1, u_1 \rangle, \ldots, \langle Bu_n, u_n\rangle)$ attains its maximum. Then for $u_i, u_j$, we want to rotate in the $i-j$ plane by angle $\theta$. Since $u_i, u_j$ span a two dimensional subspace, recall the $2$-dimensional rotation matrix.
Let 
\[
R_{\theta} = 
\begin{bmatrix}
\cos(\theta) & \sin(\theta) \\
\sin(\theta) & -\cos(\theta)
\end{bmatrix}; 
u_{i;j} = 
\begin{bmatrix}
u_i \\
u_j
\end{bmatrix}
\]
Multiplying, we get
\[
R_{\theta}u_{i;j} = 
\begin{bmatrix}
\cos(\theta) & \sin(\theta) \\
\sin(\theta) & -\cos(\theta)
\end{bmatrix}
\begin{bmatrix}
u_i \\
u_j
\end{bmatrix}
= 
\begin{bmatrix}
\cos(\theta)u_i + \sin(\theta)u_j \\
\sin(\theta)u_i -\cos(\theta)u_j
\end{bmatrix}
= 
\begin{bmatrix}
\left(R_{\theta}u_{i;j}\right)_1 \\
\left(R_{\theta}u_{i;j}\right)_2
\end{bmatrix}
\] 
Then, we replace $f$ with $g(\theta) =$ %\langle Bu_1, u_1 \rangle,
\[
f\left(\langle Bu_1, u_1 \rangle, \ldots, \langle B\left(R_{\theta}u_{i;j}\right)_1, \left(R_{\theta}u_{i;j}\right)_1 \rangle, \langle B\left(R_{\theta}u_{i;j}\right)_2, \left(R_{\theta}u_{i;j}\right)_2\rangle, \ldots, \langle Bu_n, u_n \rangle\right)
\] 
where we keep all other dot products the same. 
By assumption, $g$ attains its maximum at $\te=0$, so $g'(0) = 0, g''(0) \leq 0$. Expanding out the rotated dot products explicitly in $g(\theta)$, we get that the $i$th argument is
\[
\cos^2(\theta)\langle Bu_i, u_i\rangle + \sin^2(\theta)\langle Bu_j, u_j \rangle + \sin(2\theta)\langle Bu_i, u_j \rangle
\]
and the $j$th argument is
\[
\sin^2(\theta)\langle Bu_i, u_i\rangle + \cos^2(\theta)\langle Bu_j, u_j \rangle - \sin(2\theta)\langle Bu_i, u_j \rangle
\]
Then we can mechanically take the derivatives at $\te=0$ to get
\bal
0 &= g'(0) = 2\langle Bu_i, u_j \rangle (f_{x_i} - f_{x_j})\\
0 &\geq g''(0) = 2\left(\langle Bu_j, u_j\rangle - \langle Bu_i, u_i \rangle\right)\ub{(f_{x_i} - f_{x_j})}{=0\text{ if }\an{Bu_i,u_j}\ne 0} + 4\langle Bu_i, u_j\rangle^2\ub{\left(f_{x_ix_i} + f_{x_jx_j} - 2f_{x_ix_j}\right)}{\ge0}.
\end{align*}
This implies that for all $i\ne j$ $\langle Bu_i, u_j \rangle = 0$, which implies that for all $i$, $Bu_i = \mu_iu_i$ for some $\mu_i$. Thus any function applied to a vector of dot products is maximized at eigenvalues. 
\end{proof}

\begin{exr}\llabel{exr:kf}
 If $f:\R^n \to \R$ satisfies the conditions in Theorem~\ref{thm:eignmax} and $(u_1, \ldots, u_n)$, $(v_1, \ldots, v_n)$ are two orthonormal bases of $\R^n$, then for every $A: \R^n \to \R^n$, there exists $\pi \in S_n$, $(\ep_1, \ldots, \ep_n) \in \{\pm 1\}^n$ such that
\[
f(\langle Au_1, v_1\rangle, \langle Au_2, v_2\rangle, \ldots, \langle Au_n, v_n \rangle) \leq f(\ep_1s_{\pi(1)}(A), \ldots, \ep_ns_{\pi(n)}(A))
\]
Show that choosing $u, v$ as the singular vectors maximizes $f$ (over all pairs of orthonormal bases). 
\end{exr}
To solve this problem, you can rotate both vectors in the same direction and take derivatives, and also rotate them in opposite directions and take derivatives to get enough information to prove that the singular values are the maximum. 

Essentially, a lot of the inequalities you find in books follow from this. For instance, if you want to prove that the Schatten $p$-norm is a norm, it follows directly from this fact. 
\begin{cor}
Let $\|\cdot\|$ be a norm on $\R^n$ that is invariant under premutations and sign: 
\[
\|(x_1, \ldots, x_n)\| = \|(\ep_1x_{\pi(1)}, \ldots, \ep_nx_{\pi(n)})\|
\]
for all $\ep \in \{\pm 1\}^n$ and $\pi \in S_n$
(In the literature, we call this a \ivocab{symmetric norm}). 
This induces a norm on matrices $M_{m \times n}(\R)$ with 
\[
\|A\| = \|(s_{\pi(1)}(A), \ldots, s_{\pi(n)}(A)\|)
\]
Then the triangle inequality holds for matrices $A,B$:
\[
\|A + B\| \leq \|A\| + \|B\|.
\]
\end{cor}
\begin{proof}
%\[
%A + B = \sqrt{(A + B)^*(A + B)} \cdot W
%\]
%where $W$ is orthogonal matrix (this is just polar decomposition), and 
%\[
%|A + B| = \sqrt{(A + B)^*(A + B)}
%\]
%Let $u_1, \ldots, u_n$ be an orthonormal basis of $A+ B$. Then, 
%\[
%\|A + B\| = \|\langle |A + B|u_1, u_1 \rangle, \ldots, \langle |A + B|u_n, u_n \rangle\| = \|(\langle (A +  B)u_i, Wu_i\rangle)_{i = 1}^n \|
%\]
%\[
%\leq \|(\langle (A +  B)u_i, Wu_i\rangle)_{i = 1}^n \| + \|(Bu_i, Wu_i)\| \leq \|A\| + \|B\|
%\]
%So basically, the Schatten $p$-norm is a norm because it obeys rotational symmetries. 
%%HL: I'm confused by the proof above. I rewrote it. Check?
We have by Exercise~\ref{exr:kf}
\bal
\ve{A+B}&= \max_{(u_i)\perp,(v_i) \perp} \ve{(\an{(A+B)u_i,v_i})_{i=1}^n}\\
&\le \max_{(u_i)\perp,(v_i) \perp} \ve{(\an{(Au_i,v_i})_{i=1}^n} +
\max_{(u_i)\perp,(v_i) \perp} \ve{(\an{Bu_i,v_i})_{i=1}^n}\\
& \le \ve{A}+\ve{B}.
\end{align*}
\end{proof}

Remember Theorem~\ref{thm:eignmax}! For many results, you simply need to apply the right convex function to get the result. 

Our lemma follows from setting $f(x) = \sum_{i = 1}^k x_i$. 
%\begin{lem}
%For every $A: \R^m \to \R^n$, and every orthogonal projection $P: \R^n \to \R$ of rank $k$, 
%\[
%\txtn{Tr}(A^*AP) \leq \sum_{i = 1}^k s_i(A)^2
%\]
%where the $s_i(A)$ are the square roots of the eigenvalues of $B = A^*A$. 
%\end{lem}
\begin{proof}[Proof of Ky Fan Maximum Principle (Lemma~\ref{lem:kf})]
Take an orthonormal basis $u_1, \ldots, u_n$ of $P$  such that $u_1, \ldots, u_k$ is a basis of the range of $P$. Then
\[
\txtn{Tr}(BP) = \sum_{j = 1}^k \langle  Be_j, e_j \rangle \leq \sum_{i = 1}^k s_i(B) = \sum_{i = 1}^k s_i(A)^2
\]
\end{proof}





%%%%%%%%
%\begin{proof}
%%\fixme{This proof isn't complete. I will fix it next time.}
%We will prove that if $B:\R^m\to \R^m$ is positive semidefinite, then
%\[
%\Tr(BP)\le \sum_{i=1}^k s_i(B).
%\]
%To get the lemma, set $B=A^*A$. 
%\end{proof}

\section{Finding big subsets}
We'll present 4 lemmas for finding big subsets with certain properties. We'll put them together at the end.
\subsection{Little Grothendieck inequality}
\begin{thm}[Little Grothendieck inequality]\llabel{thm:lgi}
Fix $k,m,n\in \N$. Suppose that $T:\R^m\to \R^n$ is a linear operator. Then for every $x_1,\ldots, x_k\in \R^m$,
\[
\sumo rk \ve{Tx_r}_2^2\le \fc{\pi}2\ve{T}_{\ell_{\iy}^m \to \ell_2^n}^2 \sumo rk x_{ri}^2
\]
for some $i\in \{1,\ldots, m\}$ where $x_r=(x_{r1},\ldots, x_{rm})$.
\end{thm}
Later we will show $\fc{\pi}2$ is sharp. 

If we had only 1 vector, what does this say?
\[
\ve{Tx_1}_2\le \sfc{\pi}2\ve{T}_{\ell_{\iy}^m \to \ell_2^n}\ve{x_1}_{\iy}
\]
%The definition of the operator norm is at most ... times the \iy$ norm
We know the inequality is true for $k=1$ with constant 1, by definition of the operator norm. The theorem is true for arbitrary many vectors, losing an universal constant ($\fc{\pi}2$). After we see the proof, the example where $\fc{\pi}2$ is attained will be natural.

We give Grothendieck's original proof.

The key claim is the following.
\begin{clm}\llabel{clm:lgi}
\beq{eq:lgi1}
\sumo jm \pa{\sumo rk (T^*Tx_r)_j^2}^{\rc2}
\le \sfc{\pi}2 \ve{T}_{\ell_{\iy}^m \to \ell_2^n}\pa{\sum_{r=1}^k \ve{Tx_r}^2}^{\rc 2}.
\eeq
\end{clm}

\begin{proof}[Proof of Theorem~\ref{thm:lgi}]
Assuming Claim~\ref{clm:lgi}, we prove the theorem.
\bal
\sumo rk \ve{Tx_r}_2^2 & = \sumo rk \an{Tx_r,Tx_r}\\
&=\sumo rk \an{x_r, T^*Tx_r}\\
&=\sumo rk \sumo jm x_{rj}(T^*Tx_r)_j\\
&\le \sumo jm \pa{\sumo rk x_{rj}^2}^{\rc 2} \pa{\sumo rk (T^*Tx_r)_j^2}^{\rc 2}&\text{by Cauchy-Schwarz}\\
&\le \pa{\max_{1\le j\le m} \pa{\sumo rk x_{rj}^2}^{\rc 2}}
\pa{\sumo jm \sumo rk (T^*Tx_r)_j^2}^{\rc 2}\\
&\le \max_{1\le j\le m}\pa{\sumo ik x_{ij}^2}^{\rc 2}\sfc{\pi}2 \ve{T}_{\ell_{\iy}^m \to \ell_2^n} \pa{\sumo rk \ve{Tx_r}_2^2}^{\rc 2}\\
\sumo rk \ve{Tx_r}_2^2 & \le \fc{\pi}2 \ve{T}_{\ell_{\iy}^m \to \ell_2^n}^2 \max_j \sumo rk x_{ij}^2.
%bound by square root of multiple of same term, bootstrap.
\end{align*}
We bounded by a square root of the multiple of the same term, a bootstrapping argument. In the last step, divide and square.
%where we used Cauchy-Schwarz
\end{proof}

\begin{proof}[Proof of Claim~\ref{clm:lgi}]
Let $g_1,\ldots, g_k$ be iid standard Gaussian random variables. For every fixed $j\in \{1,\ldots, m\}$, 
\[
\sum_{r=1}^k g_r (T^* T x_r)_j.
\]
This is a Gaussian random variable with mean 0 and variance  %whatever the $L^2$ norm of these coefficients are
$\sumo rk (T^*Tx_r)_j^2$. Taking the expectation,\footnote{$\sfc{1}{2\pi} \int_{-\iy}^{\iy} |x| e^{-\fc{x^2}2} = -2\sfc{1}{2\pi} [e^{-\fc{x^2}2}]^{\iy}_0= \sfc{2}{\pi}$}
\bal
\E\ab{\sumo rk g_r(T^*T x_r)_j}
&= \pa{\sumo rk (T^*T x_r)_j^2}^{\rc 2} \sfc 2{\pi}.
\end{align*}
Sum these over $j$:
\begin{align}
\E \ba{
\sumo jm \ab{T^* (\sumo rk g_r T x_r)_j}
} & = \sfc 2\pi \sumo jm \pa{\sumo rk (T^*Tx_r)_j^2}^{\rc 2}\nonumber\\
\sumo jm \pa{\sumo rk (T^*Tx_r)_j^2}^{\rc 2}
&= \sfc{\pi}2 \E\ba{\sumo jm \ab{T^* \sumo rk g_r(Tx_r)_j}}.
\llabel{eq:lgi2}
\end{align}
Define a random sign vector $z\in \{\pm 1\}^m$ by 
\[
z_j = \sign\pa{\pa{T^*\sumo rk g_r Tx_r}_j}
\]
Then 
\bal
\sumo jm \ab{(T^* \sumo rk g Tx_r)_j} 
&=\an{z, T^* \sumo rk g_r Tx_r}\\
&= \an{Tz,\sumo rk g_r Tx_r}\\
& \le \ve{Tz}_2 \ve{\sumo rk g_r Tx_r}_2\\
%l_\iy to l_2. all most whatever norm of operator is.
&\le \ve{T}_{\ell_{\iy}^m \to \ell_2^n} \ve{\sumo rk g_r Tx_r}_2
%expointed was bounded pointwise pointwise.
\end{align*}
This is a pointwise inequality. Taking expectations and using Cauchy-Schwarz,
\beq{eq:lgi3}
\E\ba{
\sum_{j=1}^m \ab{\pa{T^* \sumo rk g_r Tx_r}_j}
}  \le \ve{T}_{\ell_{\iy}^m \to \ell_2^n}\pa{\E \ve{\sumo rk g_r Tx_r}_2^2}^{\rc 2}.
\eeq
What is the second moment? Expand:
\beq{eq:lgi4}
\E\ve{\sumo rk g_i Tx_r}_2^2 
=\E\ba{\sum_{ij} g_i g_j \an{Tx_i,Tx_j}}=\sumo rk \ve{Tx_r}_2^2.
\eeq
Chaining together~\eqref{eq:lgi2},~\eqref{eq:lgi3},~\eqref{eq:lgi4} gives the result.
%bound by L^2 norm above
\end{proof}

Why use the Gaussians? The identity characterizes the Gaussians using rotation invariance. %The expectation of the Gaussian 
%\sumo jm\sumo rk ... \sfc{\pi}2
%up to this piit use gaussians
Using other random variables gives other constants that are not sharp.

There will be lots of geometric lemmas:
\begin{itemize}
\item
A fact about restricting matrices. 
\item
Another geometric argument to give a different method for selecting subsets. 
\item 
A combinatorial lemma for selecting subsets.
\end{itemize}
Finally we'll put them together in a crazy induction.

